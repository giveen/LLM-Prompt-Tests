# LLM-Prompt-Tests

A collection of standardized prompts designed to benchmark and evaluate the coding capabilities, logic reasoning, and creative output of Large Language Models (LLMs). These are to be one-shot tests, if your LLM cannot produce it in one shot then it needs to up its game.

## ğŸ¯ Purpose
The goal of this repository is to track how different models (Local LLMs, GPT-4, Claude, Gemini, etc.) handle specific technical challenges. By providing a "Prompt" and comparing it against the "Expected Output," we can visualize the delta between model generations and test agent reliability.

## ğŸ¤– The Workflow: Agent Handoff
Thes coding results are generated using a multi-agent system within [LibreChat](https://www.librechat.ai/). Instead of a single prompt, I utilize **Agent Handoffs** to separate concerns:

* **Planner Agent:** Analyzes the initial prompt, breaks down the requirements from the prompt, and creates a step-by-step execution strategy.
* **Coding Agent:** Takes the roadmap from the Planner and implements the actual  code, focusing on syntax accuracy and performance.

This "relay" ensures that the logic is vetted before a single line of code is written, significantly reducing hallucinations.

## ğŸ“‚ Repository Structure
Each test is organized into its own directory to maintain a clear history of the prompt vs. the actual output.
EXAMPLE STRUCTURE
```text
/tests
  â”œâ”€â”€ bouncing-dvd-physics
  â”‚   â”œâ”€â”€ prompt.md          # The specific prompt used for the test
  â”‚   â”œâ”€â”€ result.py          # The code generated by the LLM
  â”‚   â””â”€â”€ output.mp4         # Video or screenshot showing the "Expected" result
  â””â”€â”€ [test-name]
      â”œâ”€â”€ prompt.md
      â””â”€â”€
```

## ğŸš€ Usage
Feel free to use these prompts to test your own local models (Ollama, LM Studio, etc.) or API-based agents. 

1. Navigate to a specific test folder.
2. Copy the content of `prompt.md`.
3. Provide the prompt to your LLM.
4. Run the resulting code and compare it to the file in the output folder to verify accuracy.

## ğŸ› ï¸ Contribution
If you have a prompt that consistently "breaks" models or reveals interesting architectural flaws in how AI writes code, feel free to open a Pull Request!
